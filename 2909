// EX-1

import tensorflow as tf 

vector_a = tf.constant([1, 2, 3], dtype=tf.float32) 
vector_b = tf.constant([4, 5, 6], dtype=tf.float32) 

result = tf.add(vector_a, vector_b) 
result1 = tf.multiply(vector_a, vector_b) 
result2 = tf.subtract(vector_a, vector_b) 
result3 = tf.divide(vector_a, vector_b) 

print("Result of vector addition:", result.numpy()) 
print("Result of vector multiply:", result1.numpy()) 
print("Result of vector subtract:", result2.numpy()) 
print("Result of vector divide:", result3.numpy())

// EX-2

import numpy as np 
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Dense, Input 
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import StandardScaler 
from sklearn.metrics import mean_squared_error 
import matplotlib.pyplot as plt 

np.random.seed(42) 
X = np.random.rand(1000, 5) 
y = 3 * X[:, 0] + 2 * X[:, 1] - 5 * X[:, 2] + np.random.randn(1000) 

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) 

scaler = StandardScaler() 
X_train = scaler.fit_transform(X_train) 
X_test = scaler.transform(X_test) 

model = Sequential() 
model.add(Input(shape=(X_train.shape[1],))) 
model.add(Dense(64, activation='relu')) 
model.add(Dense(32, activation='relu')) 
model.add(Dense(units=1))  

model.compile(optimizer='adam', loss='mse') 
history = model.fit(X_train, y_train, epochs=2, batch_size=32, validation_split=0.2, verbose=1) 
y_pred = model.predict(X_test) 

mse = mean_squared_error(y_test, y_pred) 
print(f"Mean Squared Error on Test Set: {mse}") 

plt.plot(history.history['loss'], label='Training Loss') 
plt.plot(history.history['val_loss'], label='Validation Loss') 
plt.title('Model Loss') 
plt.xlabel('Epochs') 
plt.ylabel('Loss') 
plt.legend() 
plt.show()

// EX-3

import numpy as np 
import tensorflow as tf 
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Dense, Input 

x_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) 
y_train = np.array([[0], [0], [0], [1]]) 

model = Sequential() 
model.add(Input(shape=(x_train.shape[1],))) 
model.add(Dense(64, activation='relu')) 
model.add(Dense(32, activation='relu')) 
model.add(Dense(1)) 

model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy']) 
model.fit(x_train, y_train, epochs=100, verbose=0) 

y_pred = model.predict(x_train) 

print("Test Input: ", x_train) 
print("Predicted Output: ", y_pred)

// EX-4

from tensorflow import keras 

input_size = 100  
hidden_units = 32  
output_units = 1  

model = keras.Sequential([ 
    keras.layers.Input(shape=(input_size,)), 
    keras.layers.Dense(units=hidden_units, activation='sigmoid'), 
    keras.layers.Dense(units=output_units, activation='sigmoid') 
]) 

model.compile(optimizer='adam', 
              loss='binary_crossentropy', 
              metrics=['accuracy']) 

model.summary()

// EX-5

import tensorflow as tf 
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense 
from tensorflow.keras.datasets import mnist 
from tensorflow.keras.utils import to_categorical 

(x_train, y_train), (x_test, y_test) = mnist.load_data() 
x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255 
x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255 
y_train = to_categorical(y_train, 10) 
y_test = to_categorical(y_test, 10) 

model = Sequential([ 
    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)), 
    MaxPooling2D(pool_size=(2, 2)), 
    Conv2D(64, kernel_size=(3, 3), activation='relu'), 
    MaxPooling2D(pool_size=(2, 2)), 
    Flatten(), 
    Dense(128, activation='relu'), 
    Dense(10, activation='softmax') 
]) 

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) 

model.fit(x_train, y_train, epochs=10, batch_size=128, verbose=1, validation_split=0.2) 
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2) 

print("\nTest accuracy:", test_acc)

// EX-6

import tensorflow as tf 
from tensorflow.keras.datasets import mnist 
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D 
from tensorflow.keras.optimizers import Adam 
import numpy as np 

(x_train, y_train), (x_test, y_test) = mnist.load_data() 
x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0 
x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0 
y_train = tf.keras.utils.to_categorical(y_train, 10) 
y_test = tf.keras.utils.to_categorical(y_test, 10) 

def build_and_train_model(learning_rate, batch_size, epochs): 
    model = Sequential([ 
        Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)), 
        MaxPooling2D(pool_size=(2, 2)), 
        Flatten(), 
        Dense(128, activation='relu'), 
        Dense(10, activation='softmax') 
    ]) 
    optimizer = Adam(learning_rate=learning_rate) 
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy']) 
    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1) 
    test_loss, test_accuracy = model.evaluate(x_test, y_test) 
    return test_accuracy 

learning_rates = [0.001, 0.01, 0.1] 
batch_sizes = [32, 64, 128] 
epochs = 10 

best_accuracy = 0 
best_hyperparameters = {} 

for lr in learning_rates: 
    for bs in batch_sizes: 
        accuracy = build_and_train_model(lr, bs, epochs) 
        print(f'Learning Rate: {lr}, Batch Size: {bs}, Accuracy: {accuracy}') 
        if accuracy > best_accuracy: 
            best_accuracy = accuracy 
            best_hyperparameters = {'learning_rate': lr, 'batch_size': bs} 

final_model = Sequential([ 
    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)), 
    MaxPooling2D(pool_size=(2, 2)), 
    Flatten(), 
    Dense(128, activation='relu'), 
    Dense(10, activation='softmax') 
]) 
optimizer = Adam(learning_rate=best_hyperparameters['learning_rate']) 
final_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy']) 
final_model.fit(x_train, y_train, batch_size=best_hyperparameters['batch_size'], epochs=epochs) 
final_test_loss, final_test_accuracy = final_model.evaluate(x_test, y_test) 
print(f'Final Model Test Accuracy: {final_test_accuracy}')

// EX-7

import tensorflow as tf 
from tensorflow.keras.applications import VGG16 
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Dense, Flatten 
from tensorflow.keras.optimizers import Adam 

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data() 
x_train, x_test = x_train / 255.0, x_test / 255.0 

y_train = tf.keras.utils.to_categorical(y_train, 10) 
y_test = tf.keras.utils.to_categorical(y_test, 10) 

base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3)) 

for layer in base_model.layers: 
    layer.trainable = False 

model = Sequential([ 
    base_model, 
    Flatten(), 
    Dense(256, activation='relu'), 
    Dense(10, activation='softmax') 
]) 

model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy']) 

model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.1) 

test_loss, test_accuracy = model.evaluate(x_test, y_test) 
print(f'Test Accuracy: {test_accuracy}')

// EX-8

import tensorflow as tf 
from tensorflow.keras.applications import VGG16 
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D 
from tensorflow.keras.optimizers import Adam 

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data() 
x_train, x_test = x_train / 255.0, x_test / 255.0 

y_train = tf.keras.utils.to_categorical(y_train, 10) 
y_test = tf.keras.utils.to_categorical(y_test, 10) 

base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3)) 

for layer in base_model.layers: 
    layer.trainable = False 

model = Sequential([ 
    base_model, 
    GlobalAveragePooling2D(), 
    Dense(256, activation='relu'), 
    Dense(10, activation='softmax') 
]) 

model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy']) 

model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.1) 

test_loss, test_accuracy = model.evaluate(x_test, y_test) 
print(f'Test Accuracy: {test_accuracy}')

// EX-9

import tensorflow as tf 
from tensorflow.keras.preprocessing.text import Tokenizer 
from tensorflow.keras.preprocessing.sequence import pad_sequences 
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense 
from tensorflow.keras.optimizers import Adam 
from sklearn.model_selection import train_test_split 
import numpy as np 

from tensorflow.keras.datasets import imdb 

num_words = 10000 

(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words) 

max_length = 500 
x_train = pad_sequences(x_train, maxlen=max_length) 
x_test = pad_sequences(x_test, maxlen=max_length) 

model = Sequential([ 
    Embedding(input_dim=num_words, output_dim=64, input_length=max_length), 
    SimpleRNN(64, return_sequences=False), 
    Dense(1, activation='sigmoid') 
]) 

model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy']) 

model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2) 

test_loss, test_accuracy = model.evaluate(x_test, y_test) 
print(f'Test Accuracy: {test_accuracy}')

// EX-10

import tensorflow as tf 
from tensorflow.keras.layers import Dense, Flatten, Reshape, LeakyReLU, BatchNormalization, Conv2D, Conv2DTranspose 
from tensorflow.keras.models import Sequential 
from tensorflow.keras.optimizers import Adam 
import numpy as np 
import matplotlib.pyplot as plt 

(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data() 
x_train = x_train / 255.0 
x_train = np.expand_dims(x_train, axis=-1) 

def build_generator(): 
    model = Sequential([ 
        Dense(256, input_dim=100), 
        LeakyReLU(alpha=0.2), 
        BatchNormalization(momentum=0.8), 
        Dense(512), 
        LeakyReLU(alpha=0.2), 
        BatchNormalization(momentum=0.8), 
        Dense(1024), 
        LeakyReLU(alpha=0.2), 
        BatchNormalization(momentum=0.8), 
        Dense(28*28*1, activation='tanh'), 
        Reshape((28, 28, 1)) 
    ])    
    return model 

def build_discriminator(): 
    model = Sequential([ 
        Flatten(input_shape=(28, 28, 1)), 
        Dense(512), 
        LeakyReLU(alpha=0.2), 
        Dense(256), 
        LeakyReLU(alpha=0.2), 
        Dense(1, activation='sigmoid') 
    ]) 
    return model 

discriminator = build_discriminator() 
discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy']) 

generator = build_generator() 
z = tf.keras.Input(shape=(100,)) 
img = generator(z) 
discriminator.trainable = False 
valid = discriminator(img) 
combined = tf.keras.Model(z, valid) 
combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5)) 

def train_gan(epochs, batch_size=128, save_interval=200): 
    (x_train, _), (_, _) = tf.keras.datasets.mnist.load_data() 
    x_train = (x_train.astype(np.float32) - 127.5) / 127.5 
    x_train = np.expand_dims(x_train, axis=-1) 

    valid = np.ones((batch_size, 1)) 
    fake = np.zeros((batch_size, 1)) 

    for epoch in range(epochs): 
        idx = np.random.randint(0, x_train.shape[0], batch_size) 
        imgs = x_train[idx] 

        noise = np.random.normal(0, 1, (batch_size, 100)) 
        gen_imgs = generator.predict(noise) 

        d_loss_real = discriminator.train_on_batch(imgs, valid) 
        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake) 
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) 

        noise = np.random.normal(0, 1, (batch_size, 100)) 
        g_loss = combined.train_on_batch(noise, valid) 

        print(f"{epoch} [D loss: {d_loss[0]}] [G loss: {g_loss}]") 

        if epoch % save_interval == 0: 
            save_imgs(epoch) 

def save_imgs(epoch): 
    r, c = 5, 5 
    noise = np.random.normal(0, 1, (r * c, 100)) 
    gen_imgs = generator.predict(noise) 

    gen_imgs = 0.5 * gen_imgs + 0.5 

    fig, axs = plt.subplots(r, c) 
    cnt = 0 
    for i in range(r): 
        for j in range(c): 
            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray') 
            axs[i, j].axis('off') 
            cnt += 1 
    fig.savefig(f"images/mnist_{epoch}.png") 
    plt.close() 

train_gan(epochs=10000, batch_size=64, save_interval=1000)
